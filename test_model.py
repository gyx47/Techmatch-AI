#!/usr/bin/env python3
import os
import sys
import traceback

print("=" * 60)
print("ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹å®Œæ•´æ€§")
print("=" * 60)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

# æ¨¡å‹è·¯å¾„
model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")

# 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
print("\n1. ğŸ“‹ æ£€æŸ¥å¿…è¦æ–‡ä»¶...")
required_files = [
    'config.json',
    'pytorch_model.bin',
    'tokenizer_config.json',
    'vocab.txt',
    'sentence_bert_config.json'
]

all_files_ok = True
for file in required_files:
    file_path = os.path.join(model_path, file)
    if os.path.exists(file_path):
        size = os.path.getsize(file_path)
        size_mb = size / (1024 * 1024)
        print(f"   âœ“ {file}: {size_mb:.2f} MB")
    else:
        print(f"   âœ— {file}: ä¸å­˜åœ¨ï¼")
        all_files_ok = False

if not all_files_ok:
    print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶ï¼Œè¯·é‡æ–°ä¸‹è½½")
    sys.exit(1)

# 2. æ£€æŸ¥å¤§æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
print("\n2. ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§...")
model_file = os.path.join(model_path, 'pytorch_model.bin')
file_size = os.path.getsize(model_file)
print(f"   æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")

# å°è¯•è¯»å–æ–‡ä»¶å¼€å¤´å’Œç»“å°¾
try:
    with open(model_file, 'rb') as f:
        # è¯»å–æ–‡ä»¶å¼€å¤´ï¼ˆå‰100å­—èŠ‚ï¼‰
        f.seek(0)
        header = f.read(100)
        print(f"   âœ“ èƒ½è¯»å–æ–‡ä»¶å¼€å¤´")
        
        # è¯»å–æ–‡ä»¶ç»“å°¾ï¼ˆæœ€å100å­—èŠ‚ï¼‰
        f.seek(-100, 2)  # ä»æ–‡ä»¶æœ«å°¾å‘å‰100å­—èŠ‚
        footer = f.read(100)
        print(f"   âœ“ èƒ½è¯»å–æ–‡ä»¶ç»“å°¾")
        
        if len(header) == 100 and len(footer) == 100:
            print("   âœ… æ–‡ä»¶å¯ä»¥å®Œæ•´è¯»å–")
        else:
            print("   âš ï¸ æ–‡ä»¶è¯»å–ä¸å®Œæ•´")
except Exception as e:
    print(f"   âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    sys.exit(1)

# 3. å°è¯•åŠ è½½æ¨¡å‹
print("\n3. ğŸš€ å°è¯•åŠ è½½æ¨¡å‹...")
try:
    from sentence_transformers import SentenceTransformer
    
    print("   å¯¼å…¥åº“æˆåŠŸï¼Œå¼€å§‹åŠ è½½æ¨¡å‹...")
    model = SentenceTransformer(model_path)
    print("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # 4. æµ‹è¯•ç¼–ç åŠŸèƒ½
    print("\n4. ğŸ§ª æµ‹è¯•ç¼–ç åŠŸèƒ½...")
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­",
        "This is a test sentence",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®å’Œç®—åŠ›"
    ]
    
    print(f"   æµ‹è¯•æ–‡æœ¬: {test_texts}")
    embeddings = model.encode(test_texts)
    print(f"   âœ… ç¼–ç æˆåŠŸï¼")
    print(f"   å‘é‡ç»´åº¦: {embeddings.shape}")
    print(f"   ç¬¬ä¸€ä¸ªå‘é‡æ ·ä¾‹ï¼ˆå‰5ä¸ªå€¼ï¼‰: {embeddings[0][:5]}")
    
    # 5. æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
    print("\n5. ğŸ”— æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—...")
    from sentence_transformers import util
    
    query = "æœºå™¨å­¦ä¹ "
    documents = ["æ·±åº¦å­¦ä¹ ", "äººå·¥æ™ºèƒ½", "ç¼–ç¨‹è¯­è¨€", "æ•°æ®ç§‘å­¦"]
    
    query_embedding = model.encode(query)
    doc_embeddings = model.encode(documents)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]
    
    print(f"   æŸ¥è¯¢: '{query}'")
    for i, (doc, score) in enumerate(zip(documents, cos_scores)):
        print(f"      {i+1}. '{doc}': ç›¸ä¼¼åº¦ {score:.4f}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å®Œå…¨æ­£å¸¸")
    print("=" * 60)
    
except ImportError as e:
    print(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("   è¯·å®‰è£…: pip install sentence-transformers")
    sys.exit(1)
    
except Exception as e:
    print(f"   âŒ æ¨¡å‹åŠ è½½æˆ–æµ‹è¯•å¤±è´¥: {type(e).__name__}")
    print(f"   é”™è¯¯ä¿¡æ¯: {str(e)[:200]}")
    print("\nğŸ”§ è°ƒè¯•ä¿¡æ¯:")
    traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âš ï¸ æ¨¡å‹å¯èƒ½æŸåï¼Œå»ºè®®ï¼š")
    print("   1. é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
    print("   2. æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³ï¼ˆéœ€è¦1GB+ç©ºé—²å†…å­˜ï¼‰")
    print("   3. æ¢ç”¨æ›´å°çš„æ¨¡å‹")
    print("=" * 60)